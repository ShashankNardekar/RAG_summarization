{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Document Summarization RAG using OpenAI GPT4 + LLAMAIndex + W&B\n",
    " \n",
    "This solution outlines the development of a prototype for building a Retrieval Augmented Generation (RAG) system, focusing on summarization of a large document using OpenAI GPT, LlamaIndex and Weights & Biases (W&B).\n",
    "\n",
    "It covers: setting up a local RAG system and tracking experiments."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "1. **Load a PDF Document Using LlamaIndex PDFReader:** The PDF document will be the external source for the LLM to be more context based. In this case, the PDF provides context on 'AI and Big Data in Finance'.\n",
    "2. **Segment the large PDF to chunks and embed its contents:** This process is done using an embedding model. \n",
    "3. **Save the embedded vectors using VectorStore:** These vectors can then be queried by the LLM for enriched context.\n",
    "4. **Set Up the RAG System Using LLAMAIndex ServiceContext:** ServiceContext is used to prepare the LLM for processing and retrieving information based on the embeddings generated by a pretrained embedding model.\n",
    "5. **A working RAG System:** At this point, we will have a working prototype of RAG which can summarize the PDF and answer relevant queries about the PDF using the PDF's context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "\n",
    "from dotenv import load_dotenv # to load an environment variable (API Key)\n",
    "from llama_index.llms import OpenAI\n",
    "import wandb\n",
    "\n",
    "import os\n",
    "import openai\n",
    "from pathlib import Path\n",
    "\n",
    "import warnings\n",
    "\n",
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index import VectorStoreIndex\n",
    "from llama_index import ServiceContext\n",
    "\n",
    "from llama_index.llms.llama_utils import messages_to_prompt\n",
    "from llama_index.llms.llama_utils import completion_to_prompt\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.callbacks import CallbackManager, WandbCallbackHandler\n",
    "\n",
    "from llama_index import VectorStoreIndex, download_loader\n",
    "import time\n",
    "\n",
    "from llama_index.response.notebook_utils import display_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warnings.filterwarnings(\"ignore\")\n",
    "WANDB_PROJECT = \"beakbook_project_testing_v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk Size 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Loading and reading the Document\n",
    " Loading the PDFReader using llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDFReader = download_loader(\"PDFReader\")\n",
    "loader = PDFReader()\n",
    "# documents = loader.load_data(file=Path(\"./olmo.pdf\"))\n",
    "documents = loader.load_data(file=Path(\"./Artificial-intelligence-machine-learning-big-data-in-finance.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Initialize Weights & Biases (W&B)\n",
    " Weights & Biases (W&B) is used for tracking experiments, visualizing data, and sharing insights. We initialize it here for our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Streaming LlamaIndex events to W&B at https://wandb.ai/shanxnard/beakbook_project_testing_v1/runs/3qte1lao\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: `WandbCallbackHandler` is currently in beta.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Please report any issues to https://github.com/wandb/wandb/issues with the tag `llamaindex`.\n"
     ]
    }
   ],
   "source": [
    "# Initialize W&B for tracking and visualizations\n",
    "\n",
    "wandb_args = {\"project\": WANDB_PROJECT, \"name\": \"ai_finance_rag\"}\n",
    "wandb_callback = WandbCallbackHandler(run_args=wandb_args)\n",
    "callback_manager = CallbackManager([wandb_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the GPT 4 model for setting up the RAG system\n",
    "\n",
    "Setting temperature 0.1 for introducing little variation in text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() \n",
    "openai.api_key = os.getenv(\n",
    "    \"OPENAI_API_KEY\"\n",
    ")  \n",
    "\n",
    "# llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "llm = OpenAI(model=\"gpt-4-1106-preview\", temperature=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Setup ServiceContext\n",
    " Setting up the ServiceContext with the language model and embedding model (for converting text into vector representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.739284992218018\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "\n",
    "embed_model = \"local:BAAI/bge-small-en-v1.5\" # \n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm, \n",
    "    embed_model=embed_model, \n",
    "    callback_manager=callback_manager,\n",
    "    chunk_size=chunk_size\n",
    ") # 20 seconds\n",
    "\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)\n",
    "wandb.log({\"chunking\": elapsed_time})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Create Vectorized documents (from the PDF) using VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logged trace tree to W&B.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.94628262519836\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "\n",
    "# Converting the index to a query engine for retrieval\n",
    "query_engine = index.as_query_engine()\n",
    "# ~1 minute\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)\n",
    "wandb.log({\"vectorizing\": elapsed_time})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Testing the working of this RAG system \n",
    "Asking a few questions related to the loaded documents to the RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to display responses\n",
    "\n",
    "def query_and_display(question):\n",
    "    response = query_engine.query(question)\n",
    "    display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logged trace tree to W&B.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The book titled \"Artificial Intelligence, Machine Learning, Big Data in Finance\" appears to be a comprehensive resource on the intersection of advanced technologies and the financial sector. It likely covers the impact and applications of AI, machine learning, and big data analytics in finance, exploring how these technologies are transforming financial services. The content may include discussions on regulatory challenges, innovation in financial products, risk management, and the future of finance in the context of rapid technological advancements. The presence of the OECD website suggests that the book might also address international perspectives and policy considerations related to the adoption of these technologies in the finance industry."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.923530101776123\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "query_and_display(\"Can you give me a short 6-7 lines summary of the book?\")\n",
    "\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)\n",
    "wandb.log({\"summarization_or_querying\": elapsed_time})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk Size 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize W&B for tracking and visualizations\n",
    "\n",
    "wandb_args = {\"project\": WANDB_PROJECT, \"name\": \"ai_finance_rag_1024\"}\n",
    "wandb_callback = WandbCallbackHandler(run_args=wandb_args)\n",
    "callback_manager = CallbackManager([wandb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5027825832366943\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "\n",
    "embed_model = \"local:BAAI/bge-small-en-v1.5\" # \n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm, \n",
    "    embed_model=embed_model, \n",
    "    callback_manager=callback_manager,\n",
    "    chunk_size=chunk_size\n",
    ") # 20 seconds\n",
    "\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)\n",
    "wandb.log({\"chunking\": elapsed_time})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logged trace tree to W&B.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.39982175827026\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "\n",
    "# Converting the index to a query engine for retrieval\n",
    "query_engine = index.as_query_engine()\n",
    "# ~1 minute\n",
    "\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)\n",
    "wandb.log({\"vectorizing\": elapsed_time})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logged trace tree to W&B.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The book titled \"Artificial Intelligence, Machine Learning, Big Data in Finance\" appears to explore the intersection of advanced technologies with the financial sector. It likely discusses how artificial intelligence (AI) and machine learning (ML) are transforming financial services, from risk assessment to algorithmic trading and personalized financial guidance. The book may also delve into the implications of big data analytics in finance, examining both the opportunities for innovation and the challenges related to privacy, security, and regulation. As it is referenced by the OECD, an organization that often focuses on economic development and policy, the book might also touch on the policy aspects of these technological advancements in the finance industry."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.518078565597534\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "query_and_display(\"Can you give me a short 6-7 lines summary of the book?\")\n",
    "\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)\n",
    "wandb.log({\"summarization_or_querying\": elapsed_time})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see a slight reduction in response time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closing the W&B run after queries\n",
    "wandb_callback.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Evaluation of chunk size against metrics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faithfulness\n",
    "\n",
    "This metric evaluates the factual consistency of the generated answer with the given context. Faithfulness is measured on a scale from 0 to 1, where higher scores indicate a greater degree of consistency. An answer is considered faithful if all claims made within it can be directly inferred from the provided context.\n",
    "\n",
    "#### Example\n",
    "\n",
    "- **Question:** Who invented the telephone?\n",
    "- **Context:** Alexander Graham Bell was a Scottish-born inventor, scientist, and engineer who is credited with inventing and patenting the first practical telephone on March 7, 1876.\n",
    "- **High Faithfulness Answer:** Alexander Graham Bell, a Scottish-born inventor, is credited with inventing the first practical telephone on March 7, 1876.\n",
    "- **Low Faithfulness Answer:** The telephone was invented by Alexander Graham Bell in Scotland, in April 1876.\n",
    "\n",
    "### Relevancy\n",
    "\n",
    "This metric assesses how relevant the generated answer is to the given question. It is calculated on a scale from 0 to 1, where higher values signify greater relevance. Answers that are incomplete or contain unnecessary information receive lower scores.\n",
    "\n",
    "#### Example\n",
    "\n",
    "- **Question:** What are the main ingredients in a Margherita pizza?\n",
    "- **Low Relevance Answer:** A pizza typically includes dough, tomatoes, and cheese.\n",
    "- **High Relevance Answer:** A Margherita pizza specifically uses fresh tomatoes, mozzarella cheese, fresh basil, salt, and extra-virgin olive oil as its main ingredients, reflecting its Italian origin.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Other Metrics\n",
    "\n",
    "Metrics can be categorized into three groups: Honest, Harmless, and Helpful.\n",
    "\n",
    "### Honest Evaluations - (Faithfulness and Relevancy are categorized under this, so not including them here)\n",
    "\n",
    "1. **No Answer Ratio:** The proportion of queries for which the RAG provides no answer, using phrases like \"The question cannot be answered\" or \"The answer is not in the documents\". It ranges between 0 (always provides an answer) and 1 (never provides an answer).\n",
    "2. **Recall:** Measures the frequency with which the correct document is among those retrieved, across a set of queries. It ranges from 0 (no correct document retrieved) to 1 (all correct documents retrieved).\n",
    "3. **Mean Average Precision (mAP):** The average position of correctly retrieved documents, with values ranging from 0 (no correct matches) to 1 (all top results are correct).\n",
    "4. **Mean Reciprocal Rank (MRR):** Evaluates how well the system identifies the correct answer, focusing on the ranking of the first correct answer. Higher scores indicate better performance.\n",
    "5. **Normalized Discounted Cumulative Gain (NDCG):** A ranking performance measure that focuses on the relevance and position of documents in search results. Scores range from 0.0 to 1.0, with 1.0 being the perfect score.\n",
    "\n",
    "### Harmless Evaluations - Risk metrics\n",
    "\n",
    "1. **PII Detection:** Assesses the RAG's ability to identify and secure Personally Identifiable Information, using tools for detecting and redacting sensitive information.\n",
    "2. **Toxicity:** Measures the presence of offensive or harmful language in AI responses, aiming to mitigate bias and promote respectful communication.\n",
    "3. **Stereotyping:** Evaluates outputs for language that may perpetuate harmful stereotypes, focusing on avoiding bias related to race, gender, age, etc.\n",
    "4. **Jailbreaks:** Metrics designed to prevent AI from engaging in or promoting harmful activities, ensuring advice and actions suggested are safe and responsible.\n",
    "\n",
    "### Helpful Evaluations\n",
    "\n",
    "1. **Language Mismatch:** Detects and matches the language of the user's input, ensuring responses are in the same language, measured by language detection algorithms.\n",
    "2. **Conciseness:** Evaluates the succinctness of AI responses, aiming for efficient communication without unnecessary verbosity.\n",
    "3. **Coherence:** Measures the logical flow and structure of the AI's output, ensuring it reads naturally, often requiring human judgment for assessment.\n",
    "4. **Fluency:** Assesses grammatical and syntactic correctness, with scores ranging from 1 (poor quality) to 5 (perfect grammatical correctness).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Scope \n",
    "### Extension to the Current Pipeline: \n",
    "#### Risk and Security in RAG Solutions with API Gateway\n",
    "\n",
    "- **Centralized Gateway Security:** A RAG system can be paired with an API Gateway interface for accessing a Large Language Model. For example, Amazon API Gateway enhances control and security over model access. This approach includes restricting access via API keys to minimize risks and enable secure monitoring.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
